{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8Su0IQzuNbHnMSrdmfNJv"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"9D8JVHmpvfUH"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09hyOp5W2DsE"},"source":["address = \"https://raw.githubusercontent.com/Jaeik-Jeong/STCNN/main/data/\"\n","data_csv = pd.read_csv(address+'ny_data_GAA.csv', index_col=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lt5Sy_s1vgOm"},"source":["# LSTM\n","\n","M = 18 # The number of past data for input\n","H = 6 # The number of future data for output\n","N = 67 # The number of sites\n","\n","n_layers       = 2\n","in_size        = N\n","sequence_size  = M\n","hidden_size    = N*H*2\n","out_size       = N*H\n","\n","LSTM_learning_rate = 1e-4\n","LSTM_epoch = 100\n","\n","class LSTM(nn.Module):\n","    def __init__(self):\n","        super(LSTM, self).__init__()\n","        self.rnn = nn.LSTM(in_size, hidden_size, n_layers, batch_first=True)\n","        self.fc_out = nn.Linear(hidden_size, out_size)\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=LSTM_learning_rate)\n","\n","    def forward(self, x):\n","        x, _ = self.rnn(x)\n","        out = self.fc_out(x[:,-1,:])\n","        out = out.view(-1, out_size)\n","        return out\n","        \n","    def train_net(self, x, y):\n","        x, y = torch.tensor(x,dtype=torch.float), torch.tensor(y,dtype=torch.float)\n","        loss = F.mse_loss(self.forward(x), y)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ooXRIzIKwneX"},"source":["# LSTM training\n","\n","data = np.array(data_csv)\n","data_size = np.size(data, 1)\n","\n","train_data_size  = int(data_size*0.6)\n","train_batch_size = train_data_size - M - H + 1\n","\n","batch_size = 128\n","total_batch = int((train_batch_size-1)/batch_size) + 1\n","\n","train_input  = np.zeros((train_batch_size, M, N))\n","train_output = np.zeros((train_batch_size, N*H))\n","for i in range(train_batch_size):\n","    train_input[i,:]  = np.transpose(data[:,i:i+M])\n","    train_output[i,:] = np.reshape(data[:,i+M:i+M+H], (N*H))\n","\n","\n","val_data_size    = int(data_size*0.8)\n","val_batch_size   = val_data_size - train_data_size - M - H + 1\n","\n","val_input  = np.zeros((val_batch_size, M, N))\n","val_output = np.zeros((val_batch_size, N*H))\n","for i in range(val_batch_size):\n","    val_input[i,:]  = np.transpose(data[:,train_data_size+i:train_data_size+i+M])\n","    val_output[i,:] = np.reshape(data[:,train_data_size+i+M:train_data_size+i+M+H], (N*H))\n","\n","\n","model = LSTM()\n","mse_train, mse_val = [], [] # Mean Squared Error\n","mae_train, mae_val = [], [] # Mean Absolute Error\n","for epoch in range(LSTM_epoch):\n","    for i in range(total_batch):\n","        batch_x = train_input[batch_size*i:batch_size*(i+1)]\n","        batch_y = train_output[batch_size*i:batch_size*(i+1)]\n","        model.train_net(batch_x, batch_y)\n","    \n","    train_predict = model.forward(torch.tensor(train_input, dtype=torch.float)).detach().numpy()\n","    mse_train = np.mean(np.square(train_predict - train_output))\n","    mae_train = np.mean(np.abs(train_predict - train_output))\n","    scale_train = np.mean(np.abs(train_output[1:] - train_output[:-1]))\n","    \n","    val_predict = model.forward(torch.tensor(val_input, dtype=torch.float)).detach().numpy()\n","    mse_val = np.mean(np.square(val_predict - val_output))\n","    mae_val = np.mean(np.abs(val_predict - val_output))\n","    scale_val = np.mean(np.abs(val_output[1:] - val_output[:-1]))\n","\n","    NRMSE_train = round(100*np.sqrt(mse_train),2)\n","    MAPE_train  = round(100*mae_train,2)\n","    MASE_train  = round(mae_train/scale_train,2)\n","    NRMSE_val   = round(100*np.sqrt(mse_val),2)\n","    MAPE_val    = round(100*mae_val,2)\n","    MASE_val    = round(mae_val/scale_val,2)\n","\n","    print(\"epoch: {}\".format(epoch+1))\n","    print(\"NRMSE_train: {}%\".format(NRMSE_train).ljust(25), end=\"\")\n","    print(\"MAPE_train: {}%\".format(MAPE_train).ljust(25), end=\"\")\n","    print(\"MASE_train: {}\".format(MASE_train).ljust(25))\n","    print(\"NRMSE_val: {}%\".format(NRMSE_val).ljust(25), end=\"\")\n","    print(\"MAPE_val: {}%\".format(MAPE_val).ljust(25), end=\"\")\n","    print(\"MASE_val: {}\".format(MASE_val).ljust(25))\n","    print(\"------------------------------------------------------------------------------------------\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SBxOWkrCxFWn"},"source":["# LSTM test\n","\n","test_batch_size   = data_size - val_data_size - M - H + 1\n","\n","test_input  = np.zeros((test_batch_size, M, N))\n","test_output = np.zeros((test_batch_size, N, H))\n","for i in range(test_batch_size):\n","    test_input[i,:]  = np.transpose(data[:,val_data_size+i:val_data_size+i+M])\n","    test_output[i,:] = np.reshape(data[:,val_data_size+i+M:val_data_size+i+M+H], (N, H))\n","\n","test_predict = model.forward(torch.tensor(test_input, dtype=torch.float)).detach().numpy()\n","test_predict = np.reshape(test_predict, (-1, N, H))\n","mse_test = np.mean(np.square(test_predict - test_output))\n","mae_test = np.mean(np.abs(test_predict - test_output))\n","scale_test = np.mean(np.abs(test_output[1:] - test_output[:-1]))\n","\n","NRMSE_test = round(100*np.sqrt(mse_test),2)\n","MAPE_test  = round(100*mae_test,2)\n","MASE_test  = round(mae_test/scale_test,2)\n","\n","print(\"NRMSE_test: {}%\".format(NRMSE_test).ljust(25), end=\"\")\n","print(\"MAPE_test: {}%\".format(MAPE_test).ljust(25), end=\"\")\n","print(\"MASE_test: {}\".format(MASE_test).ljust(25))\n","\n","test_output_a = np.mean(test_output, axis=1)\n","test_predict_a = np.mean(test_predict, axis=1)\n","mae_test_a = np.mean(np.abs(test_predict_a - test_output_a))\n","scale_test_a = np.mean(np.abs(test_output_a[1:] - test_output_a[:-1]))\n","\n","MAPE_test_a      = round(100*mae_test_a,2)\n","MAPE_improvement = round(100*(MAPE_test-MAPE_test_a)/MAPE_test,2)\n","\n","print(\"MAPE_test (Aggregation): {}%\".format(MAPE_test_a).ljust(36), end=\"\")\n","print(\"MAPE_test Improvement: {}%\".format(MAPE_improvement).ljust(36))"],"execution_count":null,"outputs":[]}]}