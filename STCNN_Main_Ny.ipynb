{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STCNN_Main_Ny.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WyWA_XMl4xplM8tM16QHlDH2O9N6-_a1",
      "authorship_tag": "ABX9TyNbLXox5UB0ed+t2Rl3NwZX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTz_0c627YJn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzPuFpsu2TQF"
      },
      "source": [
        "try:\r\n",
        "    address = \"/content/drive/My Drive/Doctor/Research/CNN/Colab/\"\r\n",
        "    data_csv = pd.read_csv(address+'Data_Ny.csv', index_col=0)\r\n",
        "except:\r\n",
        "    address = \"\"\r\n",
        "    data_csv = pd.read_csv(address+'Data_Ny.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHCwOw237UOA"
      },
      "source": [
        "# Space-Time Convolutional Neural Network (STCNN)\n",
        "\n",
        "M = 18 # The number of past data for input\n",
        "H = 6 # The number of future data for output\n",
        "N = 67 # The number of sites\n",
        "\n",
        "in_size       = (N, M)\n",
        "out_size      = N*H\n",
        "concat_size   = int(N/2**3)*int(M/2**3) # 3 Max Pooling Layers\n",
        "learning_rate = 1e-4\n",
        "total_epoch   = 150\n",
        "\n",
        "class STCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(STCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 128, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(128)\n",
        "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.fc    = nn.Linear(concat_size*32, out_size)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), 2))\n",
        "        x = F.relu(F.max_pool2d(self.bn2(self.conv2(x)), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
        "        x = x.view(-1, concat_size*32)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def train_net(self, x, y):\n",
        "        x, y = torch.tensor(x,dtype=torch.float), torch.tensor(y,dtype=torch.float)\n",
        "        loss = F.mse_loss(self.forward(x), y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl25dU3U7brg"
      },
      "source": [
        "# STCNN training\n",
        "\n",
        "data = np.array(data_csv)\n",
        "data_size = np.size(data, 1)\n",
        "\n",
        "train_data_size  = int(data_size*0.6)\n",
        "train_batch_size = train_data_size - M - H + 1\n",
        "\n",
        "batch_size = 128\n",
        "total_batch = int((train_batch_size-1)/batch_size) + 1\n",
        "\n",
        "train_input  = np.zeros((train_batch_size, 1, N, M))\n",
        "train_output = np.zeros((train_batch_size, N*H))\n",
        "for i in range(train_batch_size):\n",
        "    train_input[i,:]  = np.reshape(data[:,i:i+M], (1, N, M))\n",
        "    train_output[i,:] = np.reshape(data[:,i+M:i+M+H], (N*H))\n",
        "\n",
        "\n",
        "val_data_size    = int(data_size*0.8)\n",
        "val_batch_size   = val_data_size - train_data_size - M - H + 1\n",
        "\n",
        "val_input  = np.zeros((val_batch_size, 1, N, M))\n",
        "val_output = np.zeros((val_batch_size, N*H))\n",
        "for i in range(val_batch_size):\n",
        "    val_input[i,:]  = np.reshape(data[:,train_data_size+i:train_data_size+i+M], (1, N, M))\n",
        "    val_output[i,:] = np.reshape(data[:,train_data_size+i+M:train_data_size+i+M+H], (N*H))\n",
        "\n",
        "\n",
        "model = STCNN()\n",
        "mse_train, mse_val = [], [] # Mean Squared Error\n",
        "mae_train, mae_val = [], [] # Mean Absolute Error\n",
        "for epoch in range(total_epoch):\n",
        "    for i in range(total_batch):\n",
        "        batch_x = train_input[batch_size*i:batch_size*(i+1)]\n",
        "        batch_y = train_output[batch_size*i:batch_size*(i+1)]\n",
        "        model.train_net(batch_x, batch_y)\n",
        "    \n",
        "    train_predict = model.forward(torch.tensor(train_input, dtype=torch.float)).detach().numpy()\n",
        "    mse_train = np.mean(np.square(train_predict - train_output))\n",
        "    mae_train = np.mean(np.abs(train_predict - train_output))\n",
        "    scale_train = np.mean(np.abs(train_output[1:] - train_output[:-1]))\n",
        "    \n",
        "    val_predict = model.forward(torch.tensor(val_input, dtype=torch.float)).detach().numpy()\n",
        "    mse_val = np.mean(np.square(val_predict - val_output))\n",
        "    mae_val = np.mean(np.abs(val_predict - val_output))\n",
        "    scale_val = np.mean(np.abs(val_output[1:] - val_output[:-1]))\n",
        "\n",
        "    NRMSE_train = round(100*np.sqrt(mse_train),2)\n",
        "    MAPE_train  = round(100*mae_train,2)\n",
        "    MASE_train  = round(mae_train/scale_train,2)\n",
        "    NRMSE_val   = round(100*np.sqrt(mse_val),2)\n",
        "    MAPE_val    = round(100*mae_val,2)\n",
        "    MASE_val    = round(mae_val/scale_val,2)\n",
        "\n",
        "    print(\"epoch: {}\".format(epoch+1))\n",
        "    print(\"NRMSE_train: {}%\".format(NRMSE_train).ljust(25), end=\"\")\n",
        "    print(\"MAPE_train: {}%\".format(MAPE_train).ljust(25), end=\"\")\n",
        "    print(\"MASE_train: {}\".format(MASE_train).ljust(25))\n",
        "    print(\"NRMSE_val: {}%\".format(NRMSE_val).ljust(25), end=\"\")\n",
        "    print(\"MAPE_val: {}%\".format(MAPE_val).ljust(25), end=\"\")\n",
        "    print(\"MASE_val: {}\".format(MASE_val).ljust(25))\n",
        "    print(\"------------------------------------------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHXWsIeT7cPf"
      },
      "source": [
        "# STCNN test\n",
        "\n",
        "test_batch_size   = data_size - val_data_size - M - H + 1\n",
        "\n",
        "test_input  = np.zeros((test_batch_size, 1, N, M))\n",
        "test_output = np.zeros((test_batch_size, N, H))\n",
        "for i in range(test_batch_size):\n",
        "    test_input[i,:]  = np.reshape(data[:,val_data_size+i:val_data_size+i+M], (1, N, M))\n",
        "    test_output[i,:] = np.reshape(data[:,val_data_size+i+M:val_data_size+i+M+H], (N, H))\n",
        "\n",
        "test_predict = model.forward(torch.tensor(test_input, dtype=torch.float)).detach().numpy()\n",
        "test_predict = np.reshape(test_predict, (-1, N, H))\n",
        "mse_test = np.mean(np.square(test_predict - test_output))\n",
        "mae_test = np.mean(np.abs(test_predict - test_output))\n",
        "scale_test = np.mean(np.abs(test_output[1:] - test_output[:-1]))\n",
        "\n",
        "NRMSE_test = round(100*np.sqrt(mse_test),2)\n",
        "MAPE_test  = round(100*mae_test,2)\n",
        "MASE_test  = round(mae_test/scale_test,2)\n",
        "\n",
        "print(\"NRMSE_test: {}%\".format(NRMSE_test).ljust(25), end=\"\")\n",
        "print(\"MAPE_test: {}%\".format(MAPE_test).ljust(25), end=\"\")\n",
        "print(\"MASE_test: {}\".format(MASE_test).ljust(25))\n",
        "\n",
        "test_output_a = np.mean(test_output, axis=1)\n",
        "test_predict_a = np.mean(test_predict, axis=1)\n",
        "mae_test_a = np.mean(np.abs(test_predict_a - test_output_a))\n",
        "scale_test_a = np.mean(np.abs(test_output_a[1:] - test_output_a[:-1]))\n",
        "\n",
        "MAPE_test_a      = round(100*mae_test_a,2)\n",
        "MAPE_improvement = round(100*(MAPE_test-MAPE_test_a)/MAPE_test,2)\n",
        "\n",
        "print(\"MAPE_test (Aggregation): {}%\".format(MAPE_test_a).ljust(36), end=\"\")\n",
        "print(\"MAPE_test Improvement: {}%\".format(MAPE_improvement).ljust(36))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}