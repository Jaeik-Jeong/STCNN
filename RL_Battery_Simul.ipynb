{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDTV8+C9VGSSxEF8MkwUgE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSlsncuef2bw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "address = \"https://raw.githubusercontent.com/Jaeik-Jeong/RLES/main/\"\n",
        "\n",
        "data_price = pd.read_csv(address+'price.csv', index_col=0)\n",
        "data_train_csv = pd.DataFrame(data_price['Price'][:1000])\n",
        "data_val_csv   = pd.DataFrame(data_price['Price'][1000:1500])\n",
        "data_test_csv  = pd.DataFrame(data_price['Price'][1500:2000])"
      ],
      "metadata": {
        "id": "PIwP3-Y3f8hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Battery_Size = 1 #p.u.\n",
        "\n",
        "max_price = max(data_price['Price'])\n",
        "\n",
        "size_train0 = len(data_train_csv)\n",
        "size_val0   = len(data_val_csv)\n",
        "size_test0  = len(data_test_csv)\n",
        "\n",
        "price_train = list(data_train_csv['Price']/max_price)\n",
        "price_val = list(data_val_csv['Price']/max_price)\n",
        "price_test = list(data_test_csv['Price']/max_price)"
      ],
      "metadata": {
        "id": "gW4WqsfLiNbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent\n",
        "\n",
        "n_layers         = 2\n",
        "in_size          = 2\n",
        "hidden_size      = 64\n",
        "out_size         = 1\n",
        "T_horizon        = 128\n",
        "learning_rate    = 0.001\n",
        "K_epoch          = 3\n",
        "gamma            = 0.99\n",
        "lmbda            = 0.95\n",
        "eps_clip         = 0.1\n",
        "C_value          = 1\n",
        "var              = 0.1**2\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.fc_s  = nn.Linear(in_size, hidden_size)\n",
        "        self.rnn   = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc_pi = nn.Linear(hidden_size, out_size)\n",
        "        self.fc_v  = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def pi(self, x, hidden):\n",
        "        x = F.relu(self.fc_s(x))\n",
        "        x = x.view(1, -1, hidden_size)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        pi = self.fc_pi(x)\n",
        "        pi = pi.view(-1, out_size)\n",
        "        return pi, hidden\n",
        "\n",
        "    def v(self, x, hidden):\n",
        "        x = F.relu(self.fc_s(x))\n",
        "        x = x.view(1, -1, hidden_size)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        v = self.fc_v(x)\n",
        "        v = v.view(-1, 1)\n",
        "        return v"
      ],
      "metadata": {
        "id": "dhOc7gS7jnC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment\n",
        "\n",
        "E_max   = Battery_Size\n",
        "P_max   = E_max\n",
        "tdelta  = 0.5\n",
        "soc_min = 0.1\n",
        "soc_max = 0.9\n",
        "a0 = -1.031; a1 = 35; a2 = 3.685; a3 = 0.2156; a4 = 0.1178; a5 = 0.3201\n",
        "b0 = 0.1463; b1 = 30.27; b2 = 0.1037; b3 = 0.0584; b4 = 0.1747; b5 = 0.1288\n",
        "c0 = 0.1063; c1 = 62.49; c2 = 0.0437; d0 = 0.0712; d1 = 61.4; d2 = 0.0288\n",
        "N = 130*215*E_max/0.1\n",
        "beta = 10/max_price\n",
        "\n",
        "class Env():\n",
        "    def __init__(self, data):\n",
        "        self.data_imb = data\n",
        "        self.state = []\n",
        "\n",
        "    def reset(self):\n",
        "        imb = self.data_imb[0]\n",
        "        E = E_max/2\n",
        "        state = [[imb, E]]\n",
        "        self.state = state\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        imb = self.data_imb[len(self.state)]\n",
        "\n",
        "        E = self.state[-1][-1]\n",
        "        soc = E/E_max\n",
        "        Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
        "        Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
        "        Rts = c0*np.exp(-c1*soc) + c2\n",
        "        Rtl = d0*np.exp(-d1*soc) + d2\n",
        "        R   = Rs + Rts + Rtl\n",
        "\n",
        "        I_cmax = 1000000*(E_max*soc_max - E)/N/(Voc*tdelta)\n",
        "        I_dmax = 1000000*(E - E_max*soc_min)/N/(Voc*tdelta)\n",
        "        p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
        "        p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
        "\n",
        "        P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
        "        P_c = min(max(-action[0]*E_max, 0), P_max, P_cmax)\n",
        "        P_d = min(max(action[0]*E_max,  0), P_max, P_dmax)\n",
        "        p_c = 1000000*P_c/N; p_d = 1000000*P_d/N\n",
        "\n",
        "        I_c = -(Voc - np.sqrt(Voc**2 + 4*R*p_c))/(2*R)\n",
        "        I_d = (Voc - np.sqrt(Voc**2 - 4*R*p_d))/(2*R)\n",
        "        if not np.isclose(p_c, 0):\n",
        "            eff_c = (Voc*I_c)/p_c; eff_d = 1\n",
        "            E_prime = E + eff_c*P_c*tdelta\n",
        "            disp = -P_c\n",
        "        elif not np.isclose(p_d, 0):\n",
        "            eff_d = p_d/(Voc*I_d); eff_c = 1\n",
        "            E_prime = E - (1/eff_d)*P_d*tdelta\n",
        "            disp = P_d\n",
        "        else:\n",
        "            eff_c = 1; eff_d = 1\n",
        "            E_prime = E\n",
        "            disp = 0\n",
        "\n",
        "        revenue = (imb*(P_d-P_c) - beta*(P_c+P_d))*tdelta\n",
        "\n",
        "        next_state = self.state + [[imb, E_prime]]\n",
        "        reward = revenue\n",
        "        done = False\n",
        "        info = [E, -P_c, P_d, revenue]\n",
        "\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done, info"
      ],
      "metadata": {
        "id": "aQmzgh4UjqxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment 2\n",
        "\n",
        "E_max   = Battery_Size\n",
        "P_max   = E_max\n",
        "tdelta  = 0.5\n",
        "soc_min = 0.1\n",
        "soc_max = 0.9\n",
        "a0 = -1.031; a1 = 35; a2 = 3.685; a3 = 0.2156; a4 = 0.1178; a5 = 0.3201\n",
        "b0 = 0.1463; b1 = 30.27; b2 = 0.1037; b3 = 0.0584; b4 = 0.1747; b5 = 0.1288\n",
        "c0 = 0.1063; c1 = 62.49; c2 = 0.0437; d0 = 0.0712; d1 = 61.4; d2 = 0.0288\n",
        "N = 130*215*E_max/0.1\n",
        "beta = 1/max_price\n",
        "\n",
        "class Env2():\n",
        "    def __init__(self, data):\n",
        "        self.data_imb = data\n",
        "        self.state = []\n",
        "\n",
        "    def reset(self):\n",
        "        imb = self.data_imb[0]\n",
        "        E = E_max/2\n",
        "        state = [[imb, E]]\n",
        "        self.state = state\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        imb = self.data_imb[len(self.state)]\n",
        "\n",
        "        E = self.state[-1][-1]\n",
        "        soc = E/E_max\n",
        "        Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
        "        Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
        "        Rts = c0*np.exp(-c1*soc) + c2\n",
        "        Rtl = d0*np.exp(-d1*soc) + d2\n",
        "        R   = Rs + Rts + Rtl\n",
        "\n",
        "        I_cmax = 1000000*(E_max*soc_max - E)/N/(Voc*tdelta)\n",
        "        I_dmax = 1000000*(E - E_max*soc_min)/N/(Voc*tdelta)\n",
        "        p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
        "        p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
        "\n",
        "        P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
        "        P_c = min(max(-action[0]*E_max, 0), P_max, P_cmax)\n",
        "        P_d = min(max(action[0]*E_max,  0), P_max, P_dmax)\n",
        "        p_c = 1000000*P_c/N; p_d = 1000000*P_d/N\n",
        "\n",
        "        I_c = -(Voc - np.sqrt(Voc**2 + 4*R*p_c))/(2*R)\n",
        "        I_d = (Voc - np.sqrt(Voc**2 - 4*R*p_d))/(2*R)\n",
        "        if not np.isclose(p_c, 0):\n",
        "            eff_c = (Voc*I_c)/p_c; eff_d = 1\n",
        "            E_prime = E + eff_c*P_c*tdelta\n",
        "            disp = -P_c\n",
        "        elif not np.isclose(p_d, 0):\n",
        "            eff_d = p_d/(Voc*I_d); eff_c = 1\n",
        "            E_prime = E - (1/eff_d)*P_d*tdelta\n",
        "            disp = P_d\n",
        "        else:\n",
        "            eff_c = 1; eff_d = 1\n",
        "            E_prime = E\n",
        "            disp = 0\n",
        "\n",
        "        revenue = (imb*(P_d-P_c) - beta*(P_c+P_d))*tdelta\n",
        "\n",
        "        next_state = self.state + [[imb, E_prime]]\n",
        "        reward = (imb*(P_d-P_c) - beta*(P_c+P_d) - abs(P_c-max(-action[0]*E_max,0)) - abs(P_d-max(action[0]*E_max,0)))*tdelta\n",
        "        done = False\n",
        "        info = [E, -P_c, P_d, revenue]\n",
        "\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done, info"
      ],
      "metadata": {
        "id": "syB3-wMW2iqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_net(model, batch, optimizer):\n",
        "    o, H, a, r, o_prime, H_prime, done = [], [], [], [], [], [], []\n",
        "    for transition in batch[0]:\n",
        "        o.append(transition[0])\n",
        "        a.append(transition[1])\n",
        "        r.append([transition[2]])\n",
        "        o_prime.append(transition[3])\n",
        "        done.append([0]) if transition[4] else done.append([1])\n",
        "    for transition in batch[1]:\n",
        "        H.append(transition[0])\n",
        "        H_prime.append(transition[1])\n",
        "\n",
        "    o         = torch.tensor(o,dtype=torch.float)\n",
        "    H         = (H[0][0].detach(), H[0][1].detach())\n",
        "    a         = torch.tensor(a,dtype=torch.float)\n",
        "    r         = torch.tensor(r,dtype=torch.float)\n",
        "    o_prime   = torch.tensor(o_prime,dtype=torch.float)\n",
        "    H_prime   = (H_prime[0][0].detach(), H_prime[0][1].detach())\n",
        "    done      = torch.tensor(done)\n",
        "\n",
        "    pdf_old = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var*torch.eye(out_size))\n",
        "    prob_old = torch.exp(pdf_old.log_prob(a)).view(len(a),1)\n",
        "    prob_old = prob_old.detach()\n",
        "\n",
        "    v_target = r + gamma * model.v(o_prime, H_prime) * done\n",
        "    td = r + gamma * model.v(o_prime, H_prime) * done - model.v(o, H)\n",
        "    td = td.detach().numpy()\n",
        "    advantage = []\n",
        "    A = 0.0\n",
        "    for delta in td[::-1].flatten():\n",
        "        A = delta + gamma*lmbda*A\n",
        "        advantage.append([A])\n",
        "    advantage.reverse()\n",
        "    advantage = torch.tensor(advantage, dtype=torch.float)\n",
        "\n",
        "    soc = o[:,1:]/E_max\n",
        "    Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
        "    Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
        "    Rts = c0*np.exp(-c1*soc) + c2\n",
        "    Rtl = d0*np.exp(-d1*soc) + d2\n",
        "    R   = Rs + Rts + Rtl\n",
        "\n",
        "    I_cmax = 1000000*(E_max*soc_max - o[:,1:])/N/(Voc*tdelta)\n",
        "    I_dmax = 1000000*(o[:,1:] - E_max*soc_min)/N/(Voc*tdelta)\n",
        "    p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
        "    p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
        "\n",
        "    P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
        "\n",
        "    for i in range(K_epoch):\n",
        "        loss_sup = F.mse_loss(torch.maximum(-P_cmax-model.pi(o, H)[0],torch.tensor(0.0)) + torch.minimum(P_dmax-model.pi(o, H)[0],torch.tensor(0.0)), torch.tensor([[0.0] for _ in range(len(a))]))\n",
        "\n",
        "        pdf = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var*torch.eye(out_size))\n",
        "        prob = torch.exp(pdf.log_prob(a)).view(len(a),1)\n",
        "        ratio = torch.exp(torch.log(prob) - torch.log(prob_old))  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "        loss_actor = torch.min(ratio * advantage, torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage)\n",
        "        loss_critic = F.mse_loss(model.v(o, H), v_target.detach())\n",
        "        loss = -(loss_actor - C_value*loss_critic)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.mean().backward(retain_graph=True)\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "lf8Bydf34nbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_net2(model, batch, optimizer):\n",
        "    o, H, a, r, o_prime, H_prime, done = [], [], [], [], [], [], []\n",
        "    for transition in batch[0]:\n",
        "        o.append(transition[0])\n",
        "        a.append(transition[1])\n",
        "        r.append([transition[2]])\n",
        "        o_prime.append(transition[3])\n",
        "        done.append([0]) if transition[4] else done.append([1])\n",
        "    for transition in batch[1]:\n",
        "        H.append(transition[0])\n",
        "        H_prime.append(transition[1])\n",
        "\n",
        "    o         = torch.tensor(o,dtype=torch.float)\n",
        "    H         = (H[0][0].detach(), H[0][1].detach())\n",
        "    a         = torch.tensor(a,dtype=torch.float)\n",
        "    r         = torch.tensor(r,dtype=torch.float)\n",
        "    o_prime   = torch.tensor(o_prime,dtype=torch.float)\n",
        "    H_prime   = (H_prime[0][0].detach(), H_prime[0][1].detach())\n",
        "    done      = torch.tensor(done)\n",
        "\n",
        "    pdf_old = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var*torch.eye(out_size))\n",
        "    prob_old = torch.exp(pdf_old.log_prob(a)).view(len(a),1)\n",
        "    prob_old = prob_old.detach()\n",
        "\n",
        "    v_target = r + gamma * model.v(o_prime, H_prime) * done\n",
        "    td = r + gamma * model.v(o_prime, H_prime) * done - model.v(o, H)\n",
        "    td = td.detach().numpy()\n",
        "    advantage = []\n",
        "    A = 0.0\n",
        "    for delta in td[::-1].flatten():\n",
        "        A = delta + gamma*lmbda*A\n",
        "        advantage.append([A])\n",
        "    advantage.reverse()\n",
        "    advantage = torch.tensor(advantage, dtype=torch.float)\n",
        "\n",
        "    soc = o[:,1:]/E_max\n",
        "    Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
        "    Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
        "    Rts = c0*np.exp(-c1*soc) + c2\n",
        "    Rtl = d0*np.exp(-d1*soc) + d2\n",
        "    R   = Rs + Rts + Rtl\n",
        "\n",
        "    I_cmax = 1000000*(E_max*soc_max - o[:,1:])/N/(Voc*tdelta)\n",
        "    I_dmax = 1000000*(o[:,1:] - E_max*soc_min)/N/(Voc*tdelta)\n",
        "    p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
        "    p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
        "\n",
        "    P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
        "\n",
        "    for i in range(K_epoch):\n",
        "        loss_sup = F.mse_loss(torch.maximum(-P_cmax-model.pi(o, H)[0],torch.tensor(0.0)) + torch.minimum(P_dmax-model.pi(o, H)[0],torch.tensor(0.0)), torch.tensor([[0.0] for _ in range(len(a))]))\n",
        "\n",
        "        pdf = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var*torch.eye(out_size))\n",
        "        prob = torch.exp(pdf.log_prob(a)).view(len(a),1)\n",
        "        ratio = torch.exp(torch.log(prob) - torch.log(prob_old))  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "        loss_actor = torch.min(ratio * advantage, torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage)\n",
        "        loss_critic = F.mse_loss(model.v(o, H), v_target.detach())\n",
        "        loss = -(loss_actor - loss_sup - C_value*loss_critic)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.mean().backward(retain_graph=True)\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "mOu4zaueIFPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = LSTM()\n",
        "env_train = Env(price_train)\n",
        "env_val   = Env(price_val)\n",
        "env_test  = Env(price_test)\n",
        "bat_train, bat_val, bat_test = [], [], [] # SoC\n",
        "cha_train, cha_val, cha_test = [], [], [] # Charging\n",
        "dis_train, dis_val, dis_test = [], [], [] # Discharging\n",
        "rev_train, rev_val, rev_test = [], [], [] # Revenue\n",
        "total_episode = 10\n",
        "max_iteration = int(len(data_train_csv)/T_horizon)\n",
        "print_interval = 1\n",
        "\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "REV_train_list = []; REV_val_list = []; REV_test_list = []\n",
        "for n_epi in range(total_episode):\n",
        "    bat_train += [[]]; bat_val += [[]]; bat_test += [[]]\n",
        "    cha_train += [[]]; cha_val += [[]]; cha_test += [[]]\n",
        "    dis_train += [[]]; dis_val += [[]]; dis_test += [[]]\n",
        "    rev_train += [[]]; rev_val += [[]]; rev_test += [[]]\n",
        "\n",
        "    state = env_train.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for i in range(max_iteration):\n",
        "        batch = [[],[]]\n",
        "        for t in range(T_horizon):\n",
        "            pi_out, next_history = model1.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "            action = np.random.multivariate_normal(pi_out.detach().numpy()[0], var*np.identity(out_size), 1)[0].tolist()\n",
        "            next_state, reward, done, info = env_train.step(action)\n",
        "\n",
        "            batch[0].append((state[-1], action, reward, next_state[-1], done))\n",
        "            batch[1].append((history, next_history))\n",
        "            state = next_state[:]\n",
        "            history = next_history\n",
        "\n",
        "            bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "            bat_train[n_epi] += [bat]\n",
        "            cha_train[n_epi] += [cha]\n",
        "            dis_train[n_epi] += [dis]\n",
        "            rev_train[n_epi] += [revenue]\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if n_epi != 0:\n",
        "            train_net(model1, batch, optimizer)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    state = env_val.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for k in range(len(env_val.data_imb)-1):\n",
        "        pi_out, next_history = model1.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "        action = pi_out[0].tolist()\n",
        "        next_state, reward, done, info = env_val.step(action)\n",
        "\n",
        "        state = next_state[:]\n",
        "        history = next_history\n",
        "\n",
        "        bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "        bat_val[n_epi] += [bat]\n",
        "        cha_val[n_epi] += [cha]\n",
        "        dis_val[n_epi] += [dis]\n",
        "        rev_val[n_epi] += [revenue]\n",
        "\n",
        "    state = env_test.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for l in range(len(env_test.data_imb)-1):\n",
        "        pi_out, next_history = model1.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "        action = pi_out[0].tolist()\n",
        "        next_state, reward, done, info = env_test.step(action)\n",
        "\n",
        "        state = next_state[:]\n",
        "        history = next_history\n",
        "\n",
        "        bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "        bat_test[n_epi] += [bat]\n",
        "        cha_test[n_epi] += [cha]\n",
        "        dis_test[n_epi] += [dis]\n",
        "        rev_test[n_epi] += [revenue]\n",
        "\n",
        "    if (n_epi+1)%print_interval == 0:\n",
        "        REV_train = round(max_price*100*np.mean(rev_train[n_epi]),3); REV_train_list += [REV_train]\n",
        "        REV_val   = round(max_price*100*np.mean(rev_val[n_epi]),3); REV_val_list += [REV_val]\n",
        "        REV_test  = round(max_price*100*np.mean(rev_test[n_epi]),3); REV_test_list += [REV_test]\n",
        "\n",
        "        print(\"episode: {}\".format(n_epi+1))\n",
        "        print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n",
        "        print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
        "        print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
        "        print(\"------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if REV_val_list[-1] == np.max(REV_val_list):\n",
        "        torch.save(model1.state_dict(), 'MODEL1.pt')"
      ],
      "metadata": {
        "id": "sUt6nlEjlW7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1_test = LSTM()\n",
        "model1_test.load_state_dict(torch.load('MODEL1.pt'))\n",
        "\n",
        "bat_val = []; bat_test = []\n",
        "cha_val = []; cha_test = []\n",
        "dis_val = []; dis_test = []\n",
        "rev_val = []; rev_test = []\n",
        "\n",
        "state = env_val.reset()\n",
        "history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "for k in range(len(env_val.data_imb)-1):\n",
        "    pi_out, next_history = model1_test.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "    action = pi_out[0].tolist()\n",
        "    next_state, reward, done, info = env_val.step(action)\n",
        "\n",
        "    state = next_state[:]\n",
        "    history = next_history\n",
        "\n",
        "    bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "    bat_val += [bat]\n",
        "    cha_val += [cha]\n",
        "    dis_val += [dis]\n",
        "    rev_val += [revenue]\n",
        "\n",
        "state = env_test.reset()\n",
        "history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "for l in range(len(env_test.data_imb)-1):\n",
        "    pi_out, next_history = model1_test.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "    action = pi_out[0].tolist()\n",
        "    next_state, reward, done, info = env_test.step(action)\n",
        "\n",
        "    state = next_state[:]\n",
        "    history = next_history\n",
        "\n",
        "    bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "    bat_test += [bat]\n",
        "    cha_test += [cha]\n",
        "    dis_test += [dis]\n",
        "    rev_test += [revenue]\n",
        "\n",
        "REV_val   = round(max_price*100*np.mean(rev_val),3)\n",
        "REV_test  = round(max_price*100*np.mean(rev_test),3)\n",
        "print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
        "print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.plot(bat_test)"
      ],
      "metadata": {
        "id": "6dgbpwAJ_4PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = LSTM()\n",
        "env_train = Env2(price_train)\n",
        "env_val   = Env2(price_val)\n",
        "env_test  = Env2(price_test)\n",
        "bat_train, bat_val, bat_test = [], [], [] # SoC\n",
        "cha_train, cha_val, cha_test = [], [], [] # Charging\n",
        "dis_train, dis_val, dis_test = [], [], [] # Discharging\n",
        "rev_train, rev_val, rev_test = [], [], [] # Revenue\n",
        "total_episode = 100\n",
        "max_iteration = int(len(data_train_csv)/T_horizon)\n",
        "print_interval = 1\n",
        "\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "REV_train_list = []; REV_val_list = []; REV_test_list = []\n",
        "for n_epi in range(total_episode):\n",
        "    bat_train += [[]]; bat_val += [[]]; bat_test += [[]]\n",
        "    cha_train += [[]]; cha_val += [[]]; cha_test += [[]]\n",
        "    dis_train += [[]]; dis_val += [[]]; dis_test += [[]]\n",
        "    rev_train += [[]]; rev_val += [[]]; rev_test += [[]]\n",
        "\n",
        "    state = env_train.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for i in range(max_iteration):\n",
        "        batch = [[],[]]\n",
        "        for t in range(T_horizon):\n",
        "            pi_out, next_history = model2.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "            action = np.random.multivariate_normal(pi_out.detach().numpy()[0], var*np.identity(out_size), 1)[0].tolist()\n",
        "            next_state, reward, done, info = env_train.step(action)\n",
        "\n",
        "            batch[0].append((state[-1], action, reward, next_state[-1], done))\n",
        "            batch[1].append((history, next_history))\n",
        "            state = next_state[:]\n",
        "            history = next_history\n",
        "\n",
        "            bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "            bat_train[n_epi] += [bat]\n",
        "            cha_train[n_epi] += [cha]\n",
        "            dis_train[n_epi] += [dis]\n",
        "            rev_train[n_epi] += [revenue]\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if n_epi != 0:\n",
        "            train_net(model2, batch, optimizer)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    state = env_val.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for k in range(len(env_val.data_imb)-1):\n",
        "        pi_out, next_history = model2.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "        action = pi_out[0].tolist()\n",
        "        next_state, reward, done, info = env_val.step(action)\n",
        "\n",
        "        state = next_state[:]\n",
        "        history = next_history\n",
        "\n",
        "        bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "        bat_val[n_epi] += [bat]\n",
        "        cha_val[n_epi] += [cha]\n",
        "        dis_val[n_epi] += [dis]\n",
        "        rev_val[n_epi] += [revenue]\n",
        "\n",
        "    state = env_test.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for l in range(len(env_test.data_imb)-1):\n",
        "        pi_out, next_history = model2.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "        action = pi_out[0].tolist()\n",
        "        next_state, reward, done, info = env_test.step(action)\n",
        "\n",
        "        state = next_state[:]\n",
        "        history = next_history\n",
        "\n",
        "        bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "        bat_test[n_epi] += [bat]\n",
        "        cha_test[n_epi] += [cha]\n",
        "        dis_test[n_epi] += [dis]\n",
        "        rev_test[n_epi] += [revenue]\n",
        "\n",
        "    if (n_epi+1)%print_interval == 0:\n",
        "        REV_train = round(max_price*100*np.mean(rev_train[n_epi]),3); REV_train_list += [REV_train]\n",
        "        REV_val   = round(max_price*100*np.mean(rev_val[n_epi]),3); REV_val_list += [REV_val]\n",
        "        REV_test  = round(max_price*100*np.mean(rev_test[n_epi]),3); REV_test_list += [REV_test]\n",
        "\n",
        "        print(\"episode: {}\".format(n_epi+1))\n",
        "        print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n",
        "        print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
        "        print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
        "        print(\"------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if REV_val_list[-1] == np.max(REV_val_list):\n",
        "        torch.save(model2.state_dict(), 'MODEL2.pt')"
      ],
      "metadata": {
        "id": "ekfC_VR8NSQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2_test = LSTM()\n",
        "model2_test.load_state_dict(torch.load('MODEL2.pt'))\n",
        "\n",
        "bat_val = []; bat_test = []\n",
        "cha_val = []; cha_test = []\n",
        "dis_val = []; dis_test = []\n",
        "rev_val = []; rev_test = []\n",
        "\n",
        "state = env_val.reset()\n",
        "history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "for k in range(len(env_val.data_imb)-1):\n",
        "    pi_out, next_history = model2_test.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "    action = pi_out[0].tolist()\n",
        "    next_state, reward, done, info = env_val.step(action)\n",
        "\n",
        "    state = next_state[:]\n",
        "    history = next_history\n",
        "\n",
        "    bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "    bat_val += [bat]\n",
        "    cha_val += [cha]\n",
        "    dis_val += [dis]\n",
        "    rev_val += [revenue]\n",
        "\n",
        "state = env_test.reset()\n",
        "history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "for l in range(len(env_test.data_imb)-1):\n",
        "    pi_out, next_history = model2_test.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "    action = pi_out[0].tolist()\n",
        "    next_state, reward, done, info = env_test.step(action)\n",
        "\n",
        "    state = next_state[:]\n",
        "    history = next_history\n",
        "\n",
        "    bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "    bat_test += [bat]\n",
        "    cha_test += [cha]\n",
        "    dis_test += [dis]\n",
        "    rev_test += [revenue]\n",
        "\n",
        "REV_val   = round(max_price*100*np.mean(rev_val),3)\n",
        "REV_test  = round(max_price*100*np.mean(rev_test),3)\n",
        "print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
        "print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.plot(bat_test)"
      ],
      "metadata": {
        "id": "sCseqEM2Oout"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = LSTM()\n",
        "env_train = Env(price_train)\n",
        "env_val   = Env(price_val)\n",
        "env_test  = Env(price_test)\n",
        "bat_train, bat_val, bat_test = [], [], [] # SoC\n",
        "cha_train, cha_val, cha_test = [], [], [] # Charging\n",
        "dis_train, dis_val, dis_test = [], [], [] # Discharging\n",
        "rev_train, rev_val, rev_test = [], [], [] # Revenue\n",
        "total_episode = 100\n",
        "max_iteration = int(len(data_train_csv)/T_horizon)\n",
        "print_interval = 1\n",
        "\n",
        "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)\n",
        "REV_train_list = []; REV_val_list = []; REV_test_list = []\n",
        "for n_epi in range(total_episode):\n",
        "    bat_train += [[]]; bat_val += [[]]; bat_test += [[]]\n",
        "    cha_train += [[]]; cha_val += [[]]; cha_test += [[]]\n",
        "    dis_train += [[]]; dis_val += [[]]; dis_test += [[]]\n",
        "    rev_train += [[]]; rev_val += [[]]; rev_test += [[]]\n",
        "\n",
        "    state = env_train.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for i in range(max_iteration):\n",
        "        batch = [[],[]]\n",
        "        for t in range(T_horizon):\n",
        "            pi_out, next_history = model3.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "            action = np.random.multivariate_normal(pi_out.detach().numpy()[0], var*np.identity(out_size), 1)[0].tolist()\n",
        "            next_state, reward, done, info = env_train.step(action)\n",
        "\n",
        "            batch[0].append((state[-1], action, reward, next_state[-1], done))\n",
        "            batch[1].append((history, next_history))\n",
        "            state = next_state[:]\n",
        "            history = next_history\n",
        "\n",
        "            bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "            bat_train[n_epi] += [bat]\n",
        "            cha_train[n_epi] += [cha]\n",
        "            dis_train[n_epi] += [dis]\n",
        "            rev_train[n_epi] += [revenue]\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if n_epi != 0:\n",
        "            train_net2(model3, batch, optimizer)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    state = env_val.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for k in range(len(env_val.data_imb)-1):\n",
        "        pi_out, next_history = model3.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "        action = pi_out[0].tolist()\n",
        "        next_state, reward, done, info = env_val.step(action)\n",
        "\n",
        "        state = next_state[:]\n",
        "        history = next_history\n",
        "\n",
        "        bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "        bat_val[n_epi] += [bat]\n",
        "        cha_val[n_epi] += [cha]\n",
        "        dis_val[n_epi] += [dis]\n",
        "        rev_val[n_epi] += [revenue]\n",
        "\n",
        "    state = env_test.reset()\n",
        "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    for l in range(len(env_test.data_imb)-1):\n",
        "        pi_out, next_history = model2.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "        action = pi_out[0].tolist()\n",
        "        next_state, reward, done, info = env_test.step(action)\n",
        "\n",
        "        state = next_state[:]\n",
        "        history = next_history\n",
        "\n",
        "        bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "        bat_test[n_epi] += [bat]\n",
        "        cha_test[n_epi] += [cha]\n",
        "        dis_test[n_epi] += [dis]\n",
        "        rev_test[n_epi] += [revenue]\n",
        "\n",
        "    if (n_epi+1)%print_interval == 0:\n",
        "        REV_train = round(max_price*100*np.mean(rev_train[n_epi]),3); REV_train_list += [REV_train]\n",
        "        REV_val   = round(max_price*100*np.mean(rev_val[n_epi]),3); REV_val_list += [REV_val]\n",
        "        REV_test  = round(max_price*100*np.mean(rev_test[n_epi]),3); REV_test_list += [REV_test]\n",
        "\n",
        "        print(\"episode: {}\".format(n_epi+1))\n",
        "        print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n",
        "        print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
        "        print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
        "        print(\"------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    if REV_val_list[-1] == np.max(REV_val_list):\n",
        "        torch.save(model3.state_dict(), 'MODEL3.pt')"
      ],
      "metadata": {
        "id": "N4BiG4wPLMCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3_test = LSTM()\n",
        "model3_test.load_state_dict(torch.load('MODEL3.pt'))\n",
        "\n",
        "bat_val = []; bat_test = []\n",
        "cha_val = []; cha_test = []\n",
        "dis_val = []; dis_test = []\n",
        "rev_val = []; rev_test = []\n",
        "\n",
        "state = env_val.reset()\n",
        "history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "for k in range(len(env_val.data_imb)-1):\n",
        "    pi_out, next_history = model3_test.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "    action = pi_out[0].tolist()\n",
        "    next_state, reward, done, info = env_val.step(action)\n",
        "\n",
        "    state = next_state[:]\n",
        "    history = next_history\n",
        "\n",
        "    bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "    bat_val += [bat]\n",
        "    cha_val += [cha]\n",
        "    dis_val += [dis]\n",
        "    rev_val += [revenue]\n",
        "\n",
        "state = env_test.reset()\n",
        "history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "for l in range(len(env_test.data_imb)-1):\n",
        "    pi_out, next_history = model3_test.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "    action = pi_out[0].tolist()\n",
        "    next_state, reward, done, info = env_test.step(action)\n",
        "\n",
        "    state = next_state[:]\n",
        "    history = next_history\n",
        "\n",
        "    bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "    bat_test += [bat]\n",
        "    cha_test += [cha]\n",
        "    dis_test += [dis]\n",
        "    rev_test += [revenue]\n",
        "\n",
        "REV_val   = round(max_price*100*np.mean(rev_val),3)\n",
        "REV_test  = round(max_price*100*np.mean(rev_test),3)\n",
        "print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
        "print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "\n",
        "plt.plot(bat_test)"
      ],
      "metadata": {
        "id": "Q9lf4Qt1K4hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_net2(model, batch, optimizer):\n",
        "#     o, H, a, r, o_prime, H_prime, done = [], [], [], [], [], [], []\n",
        "#     for transition in batch[0]:\n",
        "#         o.append(transition[0])\n",
        "#         a.append(transition[1])\n",
        "#         r.append([transition[2]])\n",
        "#         o_prime.append(transition[3])\n",
        "#         done.append([0]) if transition[4] else done.append([1])\n",
        "#     for transition in batch[1]:\n",
        "#         H.append(transition[0])\n",
        "#         H_prime.append(transition[1])\n",
        "\n",
        "#     o         = torch.tensor(o,dtype=torch.float)\n",
        "#     H         = (H[0][0].detach(), H[0][1].detach())\n",
        "#     a         = torch.tensor(a,dtype=torch.float)\n",
        "#     r         = torch.tensor(r,dtype=torch.float)\n",
        "#     o_prime   = torch.tensor(o_prime,dtype=torch.float)\n",
        "#     H_prime   = (H_prime[0][0].detach(), H_prime[0][1].detach())\n",
        "#     done      = torch.tensor(done)\n",
        "\n",
        "#     tanh = nn.Tanh()\n",
        "#     pdf_old = torch.distributions.MultivariateNormal(tanh(model.pi(o, H)[0]), var*torch.eye(out_size))\n",
        "#     prob_old = torch.exp(pdf_old.log_prob(a)).view(len(a),1)\n",
        "#     prob_old = prob_old.detach()\n",
        "\n",
        "#     v_target = r + gamma * model.v(o_prime, H_prime) * done\n",
        "#     td = r + gamma * model.v(o_prime, H_prime) * done - model.v(o, H)\n",
        "#     td = td.detach().numpy()\n",
        "#     advantage = []\n",
        "#     A = 0.0\n",
        "#     for delta in td[::-1].flatten():\n",
        "#         A = delta + gamma*lmbda*A\n",
        "#         advantage.append([A])\n",
        "#     advantage.reverse()\n",
        "#     advantage = torch.tensor(advantage, dtype=torch.float)\n",
        "\n",
        "#     for i in range(K_epoch):\n",
        "#         pdf = torch.distributions.MultivariateNormal(tanh(model.pi(o, H)[0]), var*torch.eye(out_size))\n",
        "#         prob = torch.exp(pdf.log_prob(a)).view(len(a),1)\n",
        "#         ratio = torch.exp(torch.log(prob) - torch.log(prob_old))  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "#         loss_actor = torch.min(ratio * advantage, torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage)\n",
        "#         loss_critic = F.mse_loss(model.v(o, H), v_target.detach())\n",
        "#         loss = -(loss_actor - C_value*loss_critic)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.mean().backward(retain_graph=True)\n",
        "#         optimizer.step()"
      ],
      "metadata": {
        "id": "wjqgq5u2xN6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_net2(model, batch, optimizer):\n",
        "#     o, H, a, r, o_prime, H_prime, done = [], [], [], [], [], [], []\n",
        "#     for transition in batch[0]:\n",
        "#         o.append(transition[0])\n",
        "#         a.append(transition[1])\n",
        "#         r.append([transition[2]])\n",
        "#         o_prime.append(transition[3])\n",
        "#         done.append([0]) if transition[4] else done.append([1])\n",
        "#     for transition in batch[1]:\n",
        "#         H.append(transition[0])\n",
        "#         H_prime.append(transition[1])\n",
        "\n",
        "#     o         = torch.tensor(o,dtype=torch.float)\n",
        "#     H         = (H[0][0].detach(), H[0][1].detach())\n",
        "#     a         = torch.tensor(a,dtype=torch.float)\n",
        "#     r         = torch.tensor(r,dtype=torch.float)\n",
        "#     o_prime   = torch.tensor(o_prime,dtype=torch.float)\n",
        "#     H_prime   = (H_prime[0][0].detach(), H_prime[0][1].detach())\n",
        "#     done      = torch.tensor(done)\n",
        "\n",
        "#     soc = o[:,1:]/E_max\n",
        "#     Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
        "#     Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
        "#     Rts = c0*np.exp(-c1*soc) + c2\n",
        "#     Rtl = d0*np.exp(-d1*soc) + d2\n",
        "#     R   = Rs + Rts + Rtl\n",
        "\n",
        "#     I_cmax = 1000000*(E_max*soc_max - o[:,1:])/N/(Voc*tdelta)\n",
        "#     I_dmax = 1000000*(o[:,1:] - E_max*soc_min)/N/(Voc*tdelta)\n",
        "#     p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
        "#     p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
        "\n",
        "#     P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
        "\n",
        "#     tanh = nn.Tanh()\n",
        "#     pdf_old = torch.distributions.MultivariateNormal(tanh(model.pi(o, H)[0])*(P_cmax+P_dmax)/2+(P_dmax-P_cmax)/2, var*torch.eye(out_size))\n",
        "#     prob_old = torch.exp(pdf_old.log_prob(a)).view(len(a),1)\n",
        "#     prob_old = prob_old.detach()\n",
        "\n",
        "#     v_target = r + gamma * model.v(o_prime, H_prime) * done\n",
        "#     td = r + gamma * model.v(o_prime, H_prime) * done - model.v(o, H)\n",
        "#     td = td.detach().numpy()\n",
        "#     advantage = []\n",
        "#     A = 0.0\n",
        "#     for delta in td[::-1].flatten():\n",
        "#         A = delta + gamma*lmbda*A\n",
        "#         advantage.append([A])\n",
        "#     advantage.reverse()\n",
        "#     advantage = torch.tensor(advantage, dtype=torch.float)\n",
        "\n",
        "#     for i in range(K_epoch):\n",
        "#         loss_sup = F.mse_loss(torch.maximum(-P_cmax-model.pi(o, H)[0],torch.tensor(0.0)) + torch.minimum(P_dmax-model.pi(o, H)[0],torch.tensor(0.0)), torch.tensor([[0.0] for _ in range(len(a))]))\n",
        "\n",
        "#         pdf = torch.distributions.MultivariateNormal(tanh(model.pi(o, H)[0])*(P_cmax+P_dmax)/2+(P_dmax-P_cmax)/2, var*torch.eye(out_size))\n",
        "#         prob = torch.exp(pdf.log_prob(a)).view(len(a),1)\n",
        "#         ratio = torch.exp(torch.log(prob) - torch.log(prob_old))  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "#         loss_actor = torch.min(ratio * advantage, torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage)\n",
        "#         loss_critic = F.mse_loss(model.v(o, H), v_target.detach())\n",
        "#         loss = -(loss_actor - C_value*loss_critic)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.mean().backward(retain_graph=True)\n",
        "#         optimizer.step()\n",
        "\n",
        "#     return loss_sup"
      ],
      "metadata": {
        "id": "zgOCM-1azjrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model2 = LSTM()\n",
        "# env_train = Env(price_train)\n",
        "# env_val   = Env(price_val)\n",
        "# env_test  = Env(price_test)\n",
        "# bat_train, bat_val, bat_test = [], [], [] # Action\n",
        "# cha_train, cha_val, cha_test = [], [], [] # Charging\n",
        "# dis_train, dis_val, dis_test = [], [], [] # Discharging\n",
        "# rev_train, rev_val, rev_test = [], [], [] # Revenue\n",
        "# total_episode = 100\n",
        "# max_iteration = int(len(data_train_csv)/T_horizon)\n",
        "# print_interval = 1\n",
        "\n",
        "# optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "# for n_epi in range(total_episode):\n",
        "#     loss_sup_list = [0]\n",
        "#     bat_train += [[]]; bat_val += [[]]; bat_test += [[]]\n",
        "#     cha_train += [[]]; cha_val += [[]]; cha_test += [[]]\n",
        "#     dis_train += [[]]; dis_val += [[]]; dis_test += [[]]\n",
        "#     rev_train += [[]]; rev_val += [[]]; rev_test += [[]]\n",
        "\n",
        "#     state = env_train.reset()\n",
        "#     history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "#     for i in range(max_iteration):\n",
        "#         batch = [[],[]]\n",
        "#         for t in range(T_horizon):\n",
        "#             pi_out, next_history = model2.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "#             action = np.random.multivariate_normal(pi_out.detach().numpy()[0], var*np.identity(out_size), 1)[0].tolist()\n",
        "#             next_state, reward, done, info = env_train.step(action)\n",
        "\n",
        "#             batch[0].append((state[-1], action, reward, next_state[-1], done))\n",
        "#             batch[1].append((history, next_history))\n",
        "#             state = next_state[:]\n",
        "#             history = next_history\n",
        "\n",
        "#             bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "#             bat_train[n_epi] += [bat]\n",
        "#             cha_train[n_epi] += [cha]\n",
        "#             dis_train[n_epi] += [dis]\n",
        "#             rev_train[n_epi] += [revenue]\n",
        "#             if done:\n",
        "#                 break\n",
        "\n",
        "#         if n_epi != 0:\n",
        "#             loss_sup = train_net2(model2, batch, optimizer)\n",
        "#             loss_sup_list += [float(loss_sup)]\n",
        "#         if done:\n",
        "#             break\n",
        "\n",
        "#     state = env_val.reset()\n",
        "#     history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "#     for k in range(len(env_val.data_imb)-1):\n",
        "#         pi_out, next_history = model2.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "#         action = pi_out[0].tolist()\n",
        "#         next_state, reward, done, info = env_val.step(action)\n",
        "\n",
        "#         state = next_state[:]\n",
        "#         history = next_history\n",
        "\n",
        "#         bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "#         bat_val[n_epi] += [bat]\n",
        "#         cha_val[n_epi] += [cha]\n",
        "#         dis_val[n_epi] += [dis]\n",
        "#         rev_val[n_epi] += [revenue]\n",
        "\n",
        "#     state = env_test.reset()\n",
        "#     history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "#     for l in range(len(env_test.data_imb)-1):\n",
        "#         pi_out, next_history = model2.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
        "#         action = pi_out[0].tolist()\n",
        "#         next_state, reward, done, info = env_test.step(action)\n",
        "\n",
        "#         state = next_state[:]\n",
        "#         history = next_history\n",
        "\n",
        "#         bat = info[0]; cha = info[1]; dis = info[2]; revenue = info[3]\n",
        "#         bat_test[n_epi] += [bat]\n",
        "#         cha_test[n_epi] += [cha]\n",
        "#         dis_test[n_epi] += [dis]\n",
        "#         rev_test[n_epi] += [revenue]\n",
        "\n",
        "#     if (n_epi+1)%print_interval == 0:\n",
        "#         REV_train = round(max_price*100*np.mean(rev_train[n_epi]),3)\n",
        "#         REV_val   = round(max_price*100*np.mean(rev_val[n_epi]),3)\n",
        "#         REV_test  = round(max_price*100*np.mean(rev_test[n_epi]),3)\n",
        "\n",
        "#         print(\"episode: {}\".format(n_epi+1))\n",
        "#         print(\"loss_sup: {}\".format(round(np.mean(loss_sup_list),8)).ljust(25), end=\"\")\n",
        "#         print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n",
        "#         print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
        "#         print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
        "#         print(\"------------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "0k7PkznjPekB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}